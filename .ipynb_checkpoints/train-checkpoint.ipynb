{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Net Structure\n",
    "def Net1(inp_np_train, inp_np_test, out_np_train, out_np_test, epochs, NmU, NmNut, angles_train, angles_test, consider_angles):\n",
    "    \n",
    "    # Divide train and test\n",
    "    if consider_angles:\n",
    "        inp_np_train = np.append(np.transpose(np.expand_dims(angles_train,axis=0)),inp_np_train[:,0:NmU], axis = 1)\n",
    "        inp_np_test = np.append(np.transpose(np.expand_dims(angles_test,axis=0)),inp_np_test[:,0:NmU], axis = 1)\n",
    "    else:\n",
    "        inp_np_train = inp_np_train[:,0:NmU]\n",
    "        inp_np_test = inp_np_test[:,0:NmU]\n",
    "    out_np_train = out_np_train[:,0:NmNut]\n",
    "    out_np_test = out_np_test[:,0:NmNut]\n",
    "    \n",
    "    # Create NN Network 1\n",
    "    Nin = inp_np_train.shape[1]\n",
    "    Nout = out_np_train.shape[1]\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(Nin, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, Nout),\n",
    "    )\n",
    "    \n",
    "    # Convert to torch\n",
    "    inp_train = torch.from_numpy(inp_np_train).type(torch.float32)\n",
    "    out_train = torch.from_numpy(out_np_train).type(torch.float32)\n",
    "    inp_test = torch.from_numpy(inp_np_test).type(torch.float32)\n",
    "    out_test = torch.from_numpy(out_np_test).type(torch.float32)\n",
    "    \n",
    "    # Loss Functions\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    tplot = []\n",
    "    lossplottrain = []\n",
    "    lossplottest = []\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred_train = model(inp_train)\n",
    "        y_pred_test = model(inp_test)\n",
    "    \n",
    "        # Compute and print loss.\n",
    "        loss_train = loss_fn(y_pred_train, out_train)\n",
    "        loss_test = loss_fn(y_pred_test, out_test)\n",
    "        if t % 100 == 99:\n",
    "            print(t, \"loss on train\" , loss_train.item())\n",
    "            print(t, \"loss on test\" , loss_test.item())\n",
    "            tplot.append(t)\n",
    "            lossplottrain.append(loss_train.item())\n",
    "            lossplottest.append(loss_test.item())\n",
    "    \n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss_train.backward()\n",
    "    \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "    return tplot,lossplottrain,lossplottest,model\n",
    "\n",
    "## Input Data\n",
    "# Number of modes for U\n",
    "NmU = 10 \n",
    "# Number of modes for Nut\n",
    "NmNut = 10\n",
    "# Number of Epochs\n",
    "Epochs = 10000\n",
    "\n",
    "## Read the coefficients train\n",
    "# U\n",
    "inp_np_train_U = np.load(\"train/coeffL2U.npy\")\n",
    "# P\n",
    "inp_np_train_P = np.load(\"train/coeffL2P.npy\")\n",
    "# Nut\n",
    "out_np_train = np.load(\"train/coeffL2Nut.npy\")\n",
    "# Read Angles from file train\n",
    "angles_train = np.loadtxt(\"train/angOff_mat.txt\")\n",
    "NOffSnap = np.load(\"train/NOffSnap.npy\")\n",
    "angles_train_np = []\n",
    "# Fill the train angles\n",
    "for k,j in enumerate(NOffSnap):\n",
    "    for i in range(j):\n",
    "        angles_train_np.append(angles_train[k])\n",
    "angles_train_np = np.asarray(angles_train_np)\n",
    "\n",
    "# Read the coefficients test\n",
    "# U\n",
    "inp_np_test_U = np.load(\"test/coeffL2U.npy\")\n",
    "# P\n",
    "inp_np_test_P = np.load(\"test/coeffL2P.npy\")\n",
    "# Nut\n",
    "out_np_test = np.load(\"test/coeffL2Nut.npy\")\n",
    "# Read Angles from file test\n",
    "angles_test = np.loadtxt(\"test/angOn_mat.txt\")\n",
    "NOnSnap = np.load(\"test/NOnSnap.npy\")\n",
    "angles_test_np = []\n",
    "# Fill the train angles\n",
    "for k,j in enumerate(NOnSnap):\n",
    "    for i in range(j):\n",
    "        angles_test_np.append(angles_test[k])\n",
    "angles_test_np = np.asarray(angles_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 loss on train 0.0004931328585371375\n",
      "99 loss on test 0.0004927720292471349\n",
      "199 loss on train 3.9202964217111e-06\n",
      "199 loss on test 4.000105946033727e-06\n",
      "299 loss on train 2.5273661776736844e-06\n",
      "299 loss on test 2.5821125291258795e-06\n",
      "399 loss on train 1.7091302879634895e-06\n",
      "399 loss on test 1.7444848481318331e-06\n",
      "499 loss on train 1.2186626463517314e-06\n",
      "499 loss on test 1.2430677998054307e-06\n",
      "599 loss on train 8.437218639301136e-07\n",
      "599 loss on test 8.555542194699228e-07\n",
      "699 loss on train 6.317151246548747e-07\n",
      "699 loss on test 6.371152494466514e-07\n",
      "799 loss on train 4.969443239133398e-07\n",
      "799 loss on test 4.985256509826286e-07\n",
      "899 loss on train 3.9819215658098983e-07\n",
      "899 loss on test 3.9742664625919133e-07\n",
      "999 loss on train 3.2031874752647127e-07\n",
      "999 loss on test 3.183168928444502e-07\n",
      "1099 loss on train 2.5966375005737063e-07\n",
      "1099 loss on test 2.569726120782434e-07\n",
      "1199 loss on train 2.1263466010168486e-07\n",
      "1199 loss on test 2.0963345548352663e-07\n",
      "1299 loss on train 1.7649718131451664e-07\n",
      "1299 loss on test 1.733994849928422e-07\n",
      "1399 loss on train 1.491631707040142e-07\n",
      "1399 loss on test 1.4608350795697334e-07\n",
      "1499 loss on train 1.2861086418070045e-07\n",
      "1499 loss on test 1.256367454516294e-07\n",
      "1599 loss on train 1.1319393422581925e-07\n",
      "1599 loss on test 1.1037425906579301e-07\n",
      "1699 loss on train 1.0150600360248063e-07\n",
      "1699 loss on test 9.887530438845715e-08\n",
      "1799 loss on train 9.238587495019601e-08\n",
      "1799 loss on test 8.997082545647572e-08\n",
      "1899 loss on train 8.498497550135653e-08\n",
      "1899 loss on test 8.274334106772585e-08\n",
      "1999 loss on train 7.85529081781533e-08\n",
      "1999 loss on test 7.644219124358642e-08\n",
      "2099 loss on train 7.183430739132746e-08\n",
      "2099 loss on test 6.993005285949039e-08\n",
      "2199 loss on train 6.433628385593693e-08\n",
      "2199 loss on test 6.269329588803885e-08\n",
      "2299 loss on train 5.7684516718836676e-08\n",
      "2299 loss on test 5.612968578816435e-08\n",
      "2399 loss on train 5.2981988574174466e-08\n",
      "2399 loss on test 5.138048919661742e-08\n",
      "2499 loss on train 4.930227959221156e-08\n",
      "2499 loss on test 4.763104044513966e-08\n",
      "2599 loss on train 4.629797700772542e-08\n",
      "2599 loss on test 4.454991398006314e-08\n",
      "2699 loss on train 4.3763940027474746e-08\n",
      "2699 loss on test 4.196730785110958e-08\n",
      "2799 loss on train 4.156642674502109e-08\n",
      "2799 loss on test 3.97625754544606e-08\n",
      "2899 loss on train 3.9628481829367956e-08\n",
      "2899 loss on test 3.784824187391678e-08\n",
      "2999 loss on train 3.79071103395745e-08\n",
      "2999 loss on test 3.616996480104717e-08\n",
      "3099 loss on train 3.641551060695747e-08\n",
      "3099 loss on test 3.476016630088452e-08\n",
      "3199 loss on train 3.500094081232419e-08\n",
      "3199 loss on test 3.33675238550768e-08\n",
      "3299 loss on train 3.375903645519429e-08\n",
      "3299 loss on test 3.2168102848117996e-08\n",
      "3399 loss on train 3.263117775986757e-08\n",
      "3399 loss on test 3.108486623659701e-08\n",
      "3499 loss on train 3.159873429581239e-08\n",
      "3499 loss on test 3.009460414205023e-08\n",
      "3599 loss on train 3.065817466563203e-08\n",
      "3599 loss on test 2.9217005703685572e-08\n",
      "3699 loss on train 2.9770768961157046e-08\n",
      "3699 loss on test 2.8351749392641068e-08\n",
      "3799 loss on train 2.89582420265333e-08\n",
      "3799 loss on test 2.7580028927332023e-08\n",
      "3899 loss on train 2.81987642125614e-08\n",
      "3899 loss on test 2.6849882317492302e-08\n",
      "3999 loss on train 2.7487322640240563e-08\n",
      "3999 loss on test 2.616999950078025e-08\n",
      "4099 loss on train 2.6818451459575954e-08\n",
      "4099 loss on test 2.5530223268788177e-08\n",
      "4199 loss on train 2.61930672706967e-08\n",
      "4199 loss on test 2.491996298203958e-08\n",
      "4299 loss on train 2.559377065836088e-08\n",
      "4299 loss on test 2.4347366789356784e-08\n",
      "4399 loss on train 2.5028711547747662e-08\n",
      "4399 loss on test 2.3798740755864856e-08\n",
      "4499 loss on train 2.4472672777164917e-08\n",
      "4499 loss on test 2.3288567518875425e-08\n",
      "4599 loss on train 2.3953086625283504e-08\n",
      "4599 loss on test 2.278972210945085e-08\n",
      "4699 loss on train 2.3455346109813036e-08\n",
      "4699 loss on test 2.2308498159873125e-08\n",
      "4799 loss on train 2.2978342784085726e-08\n",
      "4799 loss on test 2.1852514464626438e-08\n",
      "4899 loss on train 2.2519584419455896e-08\n",
      "4899 loss on test 2.1413351092292032e-08\n",
      "4999 loss on train 2.207793237118949e-08\n",
      "4999 loss on test 2.098460605282071e-08\n",
      "5099 loss on train 2.1646112458029165e-08\n",
      "5099 loss on test 2.0565098068914267e-08\n",
      "5199 loss on train 2.1219369372715846e-08\n",
      "5199 loss on test 2.0146499579709598e-08\n",
      "5299 loss on train 2.081358552175061e-08\n",
      "5299 loss on test 1.975147334576377e-08\n",
      "5399 loss on train 2.042985514094653e-08\n",
      "5399 loss on test 1.9372111026427774e-08\n",
      "5499 loss on train 2.0067071559992655e-08\n",
      "5499 loss on test 1.9027229569701376e-08\n",
      "5599 loss on train 1.9711981380510224e-08\n",
      "5599 loss on test 1.8671986623530756e-08\n",
      "5699 loss on train 1.9630761016742326e-08\n",
      "5699 loss on test 1.8676132640393917e-08\n",
      "5799 loss on train 1.9045735655254248e-08\n",
      "5799 loss on test 1.80247585745974e-08\n",
      "5899 loss on train 1.877531374816499e-08\n",
      "5899 loss on test 1.7731522916619724e-08\n",
      "5999 loss on train 1.8425094339136194e-08\n",
      "5999 loss on test 1.7419788278516535e-08\n",
      "6099 loss on train 1.813502059633265e-08\n",
      "6099 loss on test 1.7127966600583022e-08\n",
      "6199 loss on train 1.7848153177624226e-08\n",
      "6199 loss on test 1.685702599729666e-08\n",
      "6299 loss on train 1.7589005807394642e-08\n",
      "6299 loss on test 1.6623321386077805e-08\n",
      "6399 loss on train 1.731077325928254e-08\n",
      "6399 loss on test 1.633481971907713e-08\n",
      "6499 loss on train 1.705789287598236e-08\n",
      "6499 loss on test 1.608881383674543e-08\n",
      "6599 loss on train 1.6814922787489195e-08\n",
      "6599 loss on test 1.5849224155317643e-08\n",
      "6699 loss on train 1.658054138431453e-08\n",
      "6699 loss on test 1.5619708193526094e-08\n",
      "6799 loss on train 1.635641133646004e-08\n",
      "6799 loss on test 1.5400562602962964e-08\n",
      "6899 loss on train 1.6178519857135143e-08\n",
      "6899 loss on test 1.5256611973768486e-08\n",
      "6999 loss on train 1.6006577396865396e-08\n",
      "6999 loss on test 1.5098603256546994e-08\n",
      "7099 loss on train 1.5726600466337004e-08\n",
      "7099 loss on test 1.4784974133874584e-08\n",
      "7199 loss on train 1.5558995869469072e-08\n",
      "7199 loss on test 1.4635899603376856e-08\n",
      "7299 loss on train 1.537157956477131e-08\n",
      "7299 loss on test 1.4439196505122709e-08\n",
      "7399 loss on train 1.543128469450039e-08\n",
      "7399 loss on test 1.4452903762673941e-08\n",
      "7499 loss on train 1.505024727066484e-08\n",
      "7499 loss on test 1.4128545444691554e-08\n",
      "7599 loss on train 1.5032156852612388e-08\n",
      "7599 loss on test 1.4153115124315718e-08\n",
      "7699 loss on train 1.4763332778500171e-08\n",
      "7699 loss on test 1.3850835145490237e-08\n",
      "7799 loss on train 1.4639127243754047e-08\n",
      "7799 loss on test 1.3729641423765315e-08\n",
      "7899 loss on train 1.4520411539820088e-08\n",
      "7899 loss on test 1.3618246974544945e-08\n",
      "7999 loss on train 1.4407341986100164e-08\n",
      "7999 loss on test 1.3509855456561581e-08\n",
      "8099 loss on train 1.4299682327134633e-08\n",
      "8099 loss on test 1.34080320179919e-08\n",
      "8199 loss on train 1.4195275177542044e-08\n",
      "8199 loss on test 1.3308698143532638e-08\n",
      "8299 loss on train 1.4103739509607749e-08\n",
      "8299 loss on test 1.3232599016532731e-08\n",
      "8399 loss on train 1.399666782475606e-08\n",
      "8399 loss on test 1.312283703924777e-08\n",
      "8499 loss on train 1.4806206038997516e-08\n",
      "8499 loss on test 1.3857124336880133e-08\n",
      "8599 loss on train 1.3811814802977551e-08\n",
      "8599 loss on test 1.2947036331922845e-08\n",
      "8699 loss on train 1.3723906455709312e-08\n",
      "8699 loss on test 1.2864079579344434e-08\n",
      "8799 loss on train 1.3656420883023657e-08\n",
      "8799 loss on test 1.2812684246910067e-08\n",
      "8899 loss on train 1.3556214817356249e-08\n",
      "8899 loss on test 1.2705103635823889e-08\n",
      "8999 loss on train 1.3475260907114261e-08\n",
      "8999 loss on test 1.2630117396383866e-08\n",
      "9099 loss on train 1.3409051646817716e-08\n",
      "9099 loss on test 1.2577421770743058e-08\n",
      "9199 loss on train 1.3320020642026975e-08\n",
      "9199 loss on test 1.2485099176728909e-08\n",
      "9299 loss on train 1.8113411215381348e-08\n",
      "9299 loss on test 1.7130988183566842e-08\n",
      "9399 loss on train 1.3171681523260759e-08\n",
      "9399 loss on test 1.2347432409853809e-08\n",
      "9499 loss on train 1.311860398089948e-08\n",
      "9499 loss on test 1.2293519091599592e-08\n",
      "9599 loss on train 1.30357884486898e-08\n",
      "9599 loss on test 1.2223203782468772e-08\n",
      "9699 loss on train 1.2961983486547979e-08\n",
      "9699 loss on test 1.2152092665473901e-08\n",
      "9799 loss on train 1.2920557956874745e-08\n",
      "9799 loss on test 1.212105793513274e-08\n",
      "9899 loss on train 1.3560143230506583e-08\n",
      "9899 loss on test 1.2719783448744693e-08\n",
      "9999 loss on train 1.2764108880958247e-08\n",
      "9999 loss on test 1.1967775215282472e-08\n"
     ]
    }
   ],
   "source": [
    "# Train a net without angles\n",
    "t_plot1, lossplottrain1, lossplottest1,model1 = Net1(inp_np_train_U, inp_np_test_U, out_np_train, out_np_test,  Epochs, NmU, NmNut, angles_train_np, angles_test_np, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 loss on train 0.00038359471363946795\n",
      "99 loss on test 0.00039918950642459095\n",
      "199 loss on train 3.232174276490696e-05\n",
      "199 loss on test 3.4714794310275465e-05\n",
      "299 loss on train 2.6998925477528246e-06\n",
      "299 loss on test 3.080348278672318e-06\n",
      "399 loss on train 1.6232060033871676e-06\n",
      "399 loss on test 1.8498517420084681e-06\n",
      "499 loss on train 1.2156394859630382e-06\n",
      "499 loss on test 1.3834147694069543e-06\n",
      "599 loss on train 9.56941335061856e-07\n",
      "599 loss on test 1.0860977681659278e-06\n",
      "699 loss on train 7.861054882596363e-07\n",
      "699 loss on test 8.768448083174007e-07\n",
      "799 loss on train 6.616047585339402e-07\n",
      "799 loss on test 7.289892209882964e-07\n",
      "899 loss on train 5.66620201425394e-07\n",
      "899 loss on test 6.197844868438551e-07\n",
      "999 loss on train 4.926887982037442e-07\n",
      "999 loss on test 5.308338018039649e-07\n",
      "1099 loss on train 4.3637703583954135e-07\n",
      "1099 loss on test 4.648725564493361e-07\n",
      "1199 loss on train 3.887951152137248e-07\n",
      "1199 loss on test 4.1087398017225496e-07\n",
      "1299 loss on train 3.491589097848191e-07\n",
      "1299 loss on test 3.6735960406986123e-07\n",
      "1399 loss on train 3.151882879137702e-07\n",
      "1399 loss on test 3.301948652278952e-07\n",
      "1499 loss on train 2.8607684043890913e-07\n",
      "1499 loss on test 2.996376053943095e-07\n",
      "1599 loss on train 2.613226115499856e-07\n",
      "1599 loss on test 2.7345416242496867e-07\n",
      "1699 loss on train 2.532168821289815e-07\n",
      "1699 loss on test 2.6545549758338893e-07\n",
      "1799 loss on train 2.2197755811248499e-07\n",
      "1799 loss on test 2.3292834328003664e-07\n",
      "1899 loss on train 2.0459526695049135e-07\n",
      "1899 loss on test 2.129765590552779e-07\n",
      "1999 loss on train 1.8931004319711064e-07\n",
      "1999 loss on test 1.9613734991708043e-07\n",
      "2099 loss on train 1.749185543076237e-07\n",
      "2099 loss on test 1.8009505708960205e-07\n",
      "2199 loss on train 1.791263599670856e-07\n",
      "2199 loss on test 1.846944286398866e-07\n",
      "2299 loss on train 1.6917020673190564e-07\n",
      "2299 loss on test 1.7344413549835735e-07\n",
      "2399 loss on train 1.5138320463847776e-07\n",
      "2399 loss on test 1.5168185996117245e-07\n",
      "2499 loss on train 1.4680939841582585e-07\n",
      "2499 loss on test 1.4573980422483146e-07\n",
      "2599 loss on train 1.3391611730639852e-07\n",
      "2599 loss on test 1.331287080574839e-07\n",
      "2699 loss on train 1.1781420994338987e-07\n",
      "2699 loss on test 1.1778368502746162e-07\n",
      "2799 loss on train 1.337341046792062e-07\n",
      "2799 loss on test 1.3465184167671396e-07\n",
      "2899 loss on train 1.7940112684300402e-07\n",
      "2899 loss on test 1.7361807636007143e-07\n",
      "2999 loss on train 1.2006518090856844e-07\n",
      "2999 loss on test 1.1776660358009394e-07\n",
      "3099 loss on train 1.1135595912037388e-07\n",
      "3099 loss on test 1.1066538263548864e-07\n",
      "3199 loss on train 1.2873177013261738e-07\n",
      "3199 loss on test 1.2766943768838246e-07\n",
      "3299 loss on train 1.0457835486477052e-07\n",
      "3299 loss on test 1.0325013732881416e-07\n",
      "3399 loss on train 9.558864633163466e-08\n",
      "3399 loss on test 9.633004083298147e-08\n",
      "3499 loss on train 1.2668743920585257e-07\n",
      "3499 loss on test 1.27341181155316e-07\n",
      "3599 loss on train 1.0533551630942384e-07\n",
      "3599 loss on test 1.0591623578193321e-07\n",
      "3699 loss on train 7.068823038025585e-08\n",
      "3699 loss on test 7.11683014742448e-08\n",
      "3799 loss on train 2.3354992606527958e-07\n",
      "3799 loss on test 2.3076670174759784e-07\n",
      "3899 loss on train 1.0450253995486491e-07\n",
      "3899 loss on test 1.049562285970751e-07\n",
      "3999 loss on train 9.281070134647962e-08\n",
      "3999 loss on test 9.444749338172187e-08\n",
      "4099 loss on train 3.467517615263205e-07\n",
      "4099 loss on test 3.4078482258337317e-07\n",
      "4199 loss on train 1.2176650443507242e-07\n",
      "4199 loss on test 1.1900232266270905e-07\n",
      "4299 loss on train 1.2507182134413597e-07\n",
      "4299 loss on test 1.2323354781074158e-07\n",
      "4399 loss on train 1.5710691059211968e-07\n",
      "4399 loss on test 1.5485467486087146e-07\n",
      "4499 loss on train 8.706685150627891e-08\n",
      "4499 loss on test 8.692210684557722e-08\n",
      "4599 loss on train 2.2785378916978516e-07\n",
      "4599 loss on test 2.2185182046996488e-07\n",
      "4699 loss on train 6.838295121269766e-07\n",
      "4699 loss on test 6.475547706941143e-07\n",
      "4799 loss on train 5.846237627338269e-07\n",
      "4799 loss on test 5.658095574290201e-07\n",
      "4899 loss on train 2.343754630373951e-07\n",
      "4899 loss on test 2.291749723326575e-07\n",
      "4999 loss on train 1.262632594034585e-07\n",
      "4999 loss on test 1.249982801709848e-07\n",
      "5099 loss on train 1.2206130861613929e-07\n",
      "5099 loss on test 1.1568810975859378e-07\n",
      "5199 loss on train 4.4826032308264985e-07\n",
      "5199 loss on test 4.4422833411772444e-07\n",
      "5299 loss on train 8.890047809018142e-08\n",
      "5299 loss on test 8.921173133558113e-08\n",
      "5399 loss on train 3.6897412769576476e-07\n",
      "5399 loss on test 3.48107164427347e-07\n",
      "5499 loss on train 2.529037885778962e-07\n",
      "5499 loss on test 2.386494770689751e-07\n",
      "5599 loss on train 2.036115802184213e-06\n",
      "5599 loss on test 1.936325816132012e-06\n",
      "5699 loss on train 2.4617568783469324e-07\n",
      "5699 loss on test 2.3509072377692064e-07\n",
      "5799 loss on train 2.5238654188797227e-07\n",
      "5799 loss on test 2.4147701083165884e-07\n",
      "5899 loss on train 7.300016591216263e-08\n",
      "5899 loss on test 7.451009764736227e-08\n",
      "5999 loss on train 1.7478726022090996e-07\n",
      "5999 loss on test 1.716737472179375e-07\n",
      "6099 loss on train 9.700701752990426e-08\n",
      "6099 loss on test 9.426815239521602e-08\n",
      "6199 loss on train 1.4584985308374598e-07\n",
      "6199 loss on test 1.419015518422384e-07\n",
      "6299 loss on train 5.666055926667468e-07\n",
      "6299 loss on test 5.482845608639764e-07\n",
      "6399 loss on train 4.282747738670878e-07\n",
      "6399 loss on test 4.1449473542343185e-07\n",
      "6499 loss on train 3.4777741575453547e-07\n",
      "6499 loss on test 3.039346267996734e-07\n",
      "6599 loss on train 2.0724125988635933e-06\n",
      "6599 loss on test 1.9507169781718403e-06\n",
      "6699 loss on train 7.03946128055577e-08\n",
      "6699 loss on test 7.104015509185047e-08\n",
      "6799 loss on train 1.3862558034816175e-07\n",
      "6799 loss on test 1.3972854162602744e-07\n",
      "6899 loss on train 1.4363446609877428e-07\n",
      "6899 loss on test 1.3003987930915173e-07\n",
      "6999 loss on train 6.9291552051709e-08\n",
      "6999 loss on test 7.007806601677657e-08\n",
      "7099 loss on train 8.844467629387509e-07\n",
      "7099 loss on test 8.387772822970874e-07\n",
      "7199 loss on train 2.4045741042755253e-07\n",
      "7199 loss on test 2.3413687699758157e-07\n",
      "7299 loss on train 1.1098109098384157e-06\n",
      "7299 loss on test 1.019534010993084e-06\n",
      "7399 loss on train 9.179670712455845e-08\n",
      "7399 loss on test 9.20264753290212e-08\n",
      "7499 loss on train 2.0174326209598803e-07\n",
      "7499 loss on test 1.9746055102132232e-07\n",
      "7599 loss on train 4.476479773529718e-07\n",
      "7599 loss on test 4.006237190878892e-07\n",
      "7699 loss on train 4.1875236433952523e-07\n",
      "7699 loss on test 3.9903397919260897e-07\n",
      "7799 loss on train 8.225089800362184e-07\n",
      "7799 loss on test 7.72464886722446e-07\n",
      "7899 loss on train 4.0474319007444137e-07\n",
      "7899 loss on test 3.906660310804e-07\n",
      "7999 loss on train 1.7918835055752425e-06\n",
      "7999 loss on test 1.6912244973354973e-06\n",
      "8099 loss on train 1.190365850334274e-07\n",
      "8099 loss on test 1.1802758592693863e-07\n",
      "8199 loss on train 6.625320025932524e-08\n",
      "8199 loss on test 6.850235934052762e-08\n",
      "8299 loss on train 3.47701643477194e-07\n",
      "8299 loss on test 3.320049017929705e-07\n",
      "8399 loss on train 1.8343826013733633e-07\n",
      "8399 loss on test 1.8103277454883937e-07\n",
      "8499 loss on train 6.053190304555756e-07\n",
      "8499 loss on test 5.858856866325368e-07\n",
      "8599 loss on train 9.787617045731167e-07\n",
      "8599 loss on test 9.123049267145689e-07\n",
      "8699 loss on train 2.9029953907411254e-07\n",
      "8699 loss on test 2.783895638458489e-07\n",
      "8799 loss on train 1.6572782612911396e-07\n",
      "8799 loss on test 1.6425261151198356e-07\n",
      "8899 loss on train 5.9007128072607884e-08\n",
      "8899 loss on test 6.07862062906861e-08\n",
      "8999 loss on train 6.643467287403837e-08\n",
      "8999 loss on test 6.73426256980747e-08\n",
      "9099 loss on train 4.5444870977462415e-08\n",
      "9099 loss on test 4.772864059532367e-08\n",
      "9199 loss on train 1.5338802938913432e-07\n",
      "9199 loss on test 1.4973534234741237e-07\n",
      "9299 loss on train 5.753295795329905e-07\n",
      "9299 loss on test 5.49925175619137e-07\n",
      "9399 loss on train 3.0028024866624037e-07\n",
      "9399 loss on test 2.9675763357772666e-07\n",
      "9499 loss on train 4.4961009848520916e-07\n",
      "9499 loss on test 4.2376319697723375e-07\n",
      "9599 loss on train 7.472530683116929e-07\n",
      "9599 loss on test 7.037513682917051e-07\n",
      "9699 loss on train 2.579947476988309e-06\n",
      "9699 loss on test 2.449343810440041e-06\n",
      "9799 loss on train 1.4156813676891034e-07\n",
      "9799 loss on test 1.3278391008952894e-07\n",
      "9899 loss on train 2.638714704517042e-07\n",
      "9899 loss on test 2.490933468379808e-07\n",
      "9999 loss on train 1.5610095260853996e-07\n",
      "9999 loss on test 1.522231656281292e-07\n"
     ]
    }
   ],
   "source": [
    "# Train a net with angles\n",
    "t_plot2, lossplottrain2, lossplottest2,model2 = Net1(inp_np_train_U, inp_np_test_U, out_np_train, out_np_test,  Epochs, NmU, NmNut, angles_train_np, angles_test_np, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_plot1, lossplottrain1, label=\"train1\")\n",
    "plt.plot(t_plot1, lossplottest1, label=\"test1\")\n",
    "plt.plot(t_plot2, lossplottrain2, label=\"train2\")\n",
    "plt.plot(t_plot2, lossplottest2, label=\"test2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_np_train_U[100,0:NmU].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x150 and 10x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-31837451d76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_np_train_U\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    746\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37_torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37_torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    746\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37_torch/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37_torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x150 and 10x64)"
     ]
    }
   ],
   "source": [
    "out_model = model1(torch.from_numpy(inp_np_train_U[100,0:Nmu]).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_torch = torch.from_numpy(out_np_train_U[100,:]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model - out_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
