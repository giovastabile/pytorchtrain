{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Net Structure\n",
    "def Net1(inp_np_train, inp_np_test, out_np_train, out_np_test, epochs, NmU, NmNut, angles_train, angles_test, consider_angles):\n",
    "    \n",
    "    # Divide train and test\n",
    "    if consider_angles:\n",
    "        inp_np_train = np.append(np.transpose(np.expand_dims(angles_train,axis=0)),inp_np_train[:,0:NmU], axis = 1)\n",
    "        inp_np_test = np.append(np.transpose(np.expand_dims(angles_test,axis=0)),inp_np_test[:,0:NmU], axis = 1)\n",
    "    else:\n",
    "        inp_np_train = inp_np_train[:,0:NmU]\n",
    "        inp_np_test = inp_np_test[:,0:NmU]\n",
    "    out_np_train = out_np_train[:,0:NmNut]\n",
    "    out_np_test = out_np_test[:,0:NmNut]\n",
    "    \n",
    "    # Create NN Network 1\n",
    "    Nin = inp_np_train.shape[1]\n",
    "    Nout = out_np_train.shape[1]\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(Nin, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, Nout),\n",
    "    )\n",
    "    \n",
    "    # Convert to torch\n",
    "    inp_train = torch.from_numpy(inp_np_train).type(torch.float32)\n",
    "    out_train = torch.from_numpy(out_np_train).type(torch.float32)\n",
    "    inp_test = torch.from_numpy(inp_np_test).type(torch.float32)\n",
    "    out_test = torch.from_numpy(out_np_test).type(torch.float32)\n",
    "    \n",
    "    # Loss Functions\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    tplot = []\n",
    "    lossplottrain = []\n",
    "    lossplottest = []\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred_train = model(inp_train)\n",
    "        y_pred_test = model(inp_test)\n",
    "    \n",
    "        # Compute and print loss.\n",
    "        loss_train = loss_fn(y_pred_train, out_train)\n",
    "        loss_test = loss_fn(y_pred_test, out_test)\n",
    "        if t % 100 == 99:\n",
    "            print(t, \"loss on train\" , loss_train.item())\n",
    "            print(t, \"loss on test\" , loss_test.item())\n",
    "            tplot.append(t)\n",
    "            lossplottrain.append(loss_train.item())\n",
    "            lossplottest.append(loss_test.item())\n",
    "    \n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss_train.backward()\n",
    "    \n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "    return tplot,lossplottrain,lossplottest,model\n",
    "\n",
    "## Input Data\n",
    "# Number of modes for U\n",
    "NmU = 10 \n",
    "# Number of modes for Nut\n",
    "NmNut = 10\n",
    "# Number of Epochs\n",
    "Epochs = 10000\n",
    "\n",
    "## Read the coefficients train\n",
    "# U\n",
    "inp_np_train_U = np.load(\"train/coeffL2U.npy\")\n",
    "# P\n",
    "inp_np_train_P = np.load(\"train/coeffL2P.npy\")\n",
    "# Nut\n",
    "out_np_train = np.load(\"train/coeffL2Nut.npy\")\n",
    "# Read Angles from file train\n",
    "angles_train = np.loadtxt(\"train/angOff_mat.txt\")\n",
    "NOffSnap = np.load(\"train/NOffSnap.npy\")\n",
    "angles_train_np = []\n",
    "# Fill the train angles\n",
    "for k,j in enumerate(NOffSnap):\n",
    "    for i in range(j):\n",
    "        angles_train_np.append(angles_train[k])\n",
    "angles_train_np = np.asarray(angles_train_np)\n",
    "\n",
    "# Read the coefficients test\n",
    "# U\n",
    "inp_np_test_U = np.load(\"test/coeffL2U.npy\")\n",
    "# P\n",
    "inp_np_test_P = np.load(\"test/coeffL2P.npy\")\n",
    "# Nut\n",
    "out_np_test = np.load(\"test/coeffL2Nut.npy\")\n",
    "# Read Angles from file test\n",
    "angles_test = np.loadtxt(\"test/angOn_mat.txt\")\n",
    "NOnSnap = np.load(\"test/NOnSnap.npy\")\n",
    "angles_test_np = []\n",
    "# Fill the train angles\n",
    "for k,j in enumerate(NOnSnap):\n",
    "    for i in range(j):\n",
    "        angles_test_np.append(angles_test[k])\n",
    "angles_test_np = np.asarray(angles_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 loss on train 0.023991625756025314\n",
      "99 loss on test 0.00924385990947485\n",
      "199 loss on train 0.013006212189793587\n",
      "199 loss on test 0.004953916184604168\n",
      "299 loss on train 0.009432208724319935\n",
      "299 loss on test 0.0035852682776749134\n",
      "399 loss on train 0.007152028381824493\n",
      "399 loss on test 0.0027133063413202763\n",
      "499 loss on train 0.005554786883294582\n",
      "499 loss on test 0.0021028832998126745\n",
      "599 loss on train 0.004232446197420359\n",
      "599 loss on test 0.0015969999367371202\n",
      "699 loss on train 0.003058489877730608\n",
      "699 loss on test 0.001146786380559206\n",
      "799 loss on train 0.0023021590895950794\n",
      "799 loss on test 0.000862243352457881\n",
      "899 loss on train 0.0018633462022989988\n",
      "899 loss on test 0.0006977536249905825\n",
      "999 loss on train 0.0015811248449608684\n",
      "999 loss on test 0.0005924816359765828\n",
      "1099 loss on train 0.0013751593651250005\n",
      "1099 loss on test 0.000515459687449038\n",
      "1199 loss on train 0.0012096022255718708\n",
      "1199 loss on test 0.0004531869199126959\n",
      "1299 loss on train 0.0010698499390855432\n",
      "1299 loss on test 0.0004004279908258468\n",
      "1399 loss on train 0.0009499990846961737\n",
      "1399 loss on test 0.00035506804124452174\n",
      "1499 loss on train 0.0008467347361147404\n",
      "1499 loss on test 0.0003158480394631624\n",
      "1599 loss on train 0.0007574718911200762\n",
      "1599 loss on test 0.0002819802612066269\n",
      "1699 loss on train 0.0006805104785598814\n",
      "1699 loss on test 0.0002528131881263107\n",
      "1799 loss on train 0.0006145556690171361\n",
      "1799 loss on test 0.0002279270120197907\n",
      "1899 loss on train 0.0005583725869655609\n",
      "1899 loss on test 0.0002067440073005855\n",
      "1999 loss on train 0.0005105867749080062\n",
      "1999 loss on test 0.00018881421419791877\n",
      "2099 loss on train 0.00046990468399599195\n",
      "2099 loss on test 0.00017363863298669457\n",
      "2199 loss on train 0.0004352850664872676\n",
      "2199 loss on test 0.00016077733016572893\n",
      "2299 loss on train 0.00040631103911437094\n",
      "2299 loss on test 0.000150102045154199\n",
      "2399 loss on train 0.00038105162093415856\n",
      "2399 loss on test 0.0001408141542924568\n",
      "2499 loss on train 0.0003597546892706305\n",
      "2499 loss on test 0.0001330639934167266\n",
      "2599 loss on train 0.0003413537342566997\n",
      "2599 loss on test 0.0001263174635823816\n",
      "2699 loss on train 0.00032522869878448546\n",
      "2699 loss on test 0.00012041564332321286\n",
      "2799 loss on train 0.00031100312480702996\n",
      "2799 loss on test 0.0001152124023064971\n",
      "2899 loss on train 0.00029772025300189853\n",
      "2899 loss on test 0.00011042060941690579\n",
      "2999 loss on train 0.0002854224294424057\n",
      "2999 loss on test 0.0001058790076058358\n",
      "3099 loss on train 0.00027481449069455266\n",
      "3099 loss on test 0.00010186237341258675\n",
      "3199 loss on train 0.000256774335866794\n",
      "3199 loss on test 9.694480104371905e-05\n",
      "3299 loss on train 0.00023424089886248112\n",
      "3299 loss on test 8.865287963999435e-05\n",
      "3399 loss on train 0.0002249540266348049\n",
      "3399 loss on test 8.48916097311303e-05\n",
      "3499 loss on train 0.00021718327479902655\n",
      "3499 loss on test 8.178680582204834e-05\n",
      "3599 loss on train 0.0002103344741044566\n",
      "3599 loss on test 7.90831254562363e-05\n",
      "3699 loss on train 0.00020407134434208274\n",
      "3699 loss on test 7.658702816115692e-05\n",
      "3799 loss on train 0.00019822848844341934\n",
      "3799 loss on test 7.430892583215609e-05\n",
      "3899 loss on train 0.00019265143782831728\n",
      "3899 loss on test 7.209345494629815e-05\n",
      "3999 loss on train 0.0001884720695670694\n",
      "3999 loss on test 7.049562555039302e-05\n",
      "4099 loss on train 0.00018244866805616766\n",
      "4099 loss on test 6.805964949307963e-05\n",
      "4199 loss on train 0.0001775093551259488\n",
      "4199 loss on test 6.604428926948458e-05\n",
      "4299 loss on train 0.00017662653408478945\n",
      "4299 loss on test 6.54969917377457e-05\n",
      "4399 loss on train 0.00016801862511783838\n",
      "4399 loss on test 6.238623609533533e-05\n",
      "4499 loss on train 0.00016353785758838058\n",
      "4499 loss on test 6.070486415410414e-05\n",
      "4599 loss on train 0.0001589770254213363\n",
      "4599 loss on test 5.882636469323188e-05\n",
      "4699 loss on train 0.00015448209887836128\n",
      "4699 loss on test 5.702305134036578e-05\n",
      "4799 loss on train 0.00014989534975029528\n",
      "4799 loss on test 5.525032975128852e-05\n",
      "4899 loss on train 0.0001453411823604256\n",
      "4899 loss on test 5.353373126126826e-05\n",
      "4999 loss on train 0.00014087697491049767\n",
      "4999 loss on test 5.1813534810207784e-05\n",
      "5099 loss on train 0.0001367060176562518\n",
      "5099 loss on test 5.023385165259242e-05\n",
      "5199 loss on train 0.000132593180751428\n",
      "5199 loss on test 4.8609588702674955e-05\n",
      "5299 loss on train 0.00012854980013798922\n",
      "5299 loss on test 4.7071054723346606e-05\n",
      "5399 loss on train 0.00012467839405871928\n",
      "5399 loss on test 4.554535189527087e-05\n",
      "5499 loss on train 0.00012066021736245602\n",
      "5499 loss on test 4.40637486462947e-05\n",
      "5599 loss on train 0.00011706792429322377\n",
      "5599 loss on test 4.265988536644727e-05\n",
      "5699 loss on train 0.00011337137402733788\n",
      "5699 loss on test 4.128253931412473e-05\n",
      "5799 loss on train 0.00010992924217134714\n",
      "5799 loss on test 3.9973507227841765e-05\n",
      "5899 loss on train 0.00010676166857592762\n",
      "5899 loss on test 3.884209581883624e-05\n",
      "5999 loss on train 0.0001038729606079869\n",
      "5999 loss on test 3.770025068661198e-05\n",
      "6099 loss on train 0.00010067065159091726\n",
      "6099 loss on test 3.656815897556953e-05\n",
      "6199 loss on train 9.792187483981252e-05\n",
      "6199 loss on test 3.5565142752602696e-05\n",
      "6299 loss on train 9.548705565975979e-05\n",
      "6299 loss on test 3.4744891308946535e-05\n",
      "6399 loss on train 9.301966201746836e-05\n",
      "6399 loss on test 3.373362778802402e-05\n",
      "6499 loss on train 9.043494355864823e-05\n",
      "6499 loss on test 3.283070691395551e-05\n",
      "6599 loss on train 8.817038906272501e-05\n",
      "6599 loss on test 3.198899867129512e-05\n",
      "6699 loss on train 8.593835809733719e-05\n",
      "6699 loss on test 3.1177445634966716e-05\n",
      "6799 loss on train 8.37692350614816e-05\n",
      "6799 loss on test 3.0335319024743512e-05\n",
      "6899 loss on train 8.335759048350155e-05\n",
      "6899 loss on test 3.0329030778375454e-05\n",
      "6999 loss on train 7.978893700055778e-05\n",
      "6999 loss on test 2.88418959826231e-05\n",
      "7099 loss on train 7.824001659173518e-05\n",
      "7099 loss on test 2.819780092977453e-05\n",
      "7199 loss on train 7.619616371812299e-05\n",
      "7199 loss on test 2.7438476536190137e-05\n",
      "7299 loss on train 7.451740384567529e-05\n",
      "7299 loss on test 2.6782003260450438e-05\n",
      "7399 loss on train 0.00010270136408507824\n",
      "7399 loss on test 3.807970279012807e-05\n",
      "7499 loss on train 7.13399494998157e-05\n",
      "7499 loss on test 2.5536943212500773e-05\n",
      "7599 loss on train 7.444674702128395e-05\n",
      "7599 loss on test 2.661032704054378e-05\n",
      "7699 loss on train 6.848468910902739e-05\n",
      "7699 loss on test 2.4411021513515152e-05\n",
      "7799 loss on train 6.847245822427794e-05\n",
      "7799 loss on test 2.4339758965652436e-05\n",
      "7899 loss on train 6.586775270989165e-05\n",
      "7899 loss on test 2.3374013835564256e-05\n",
      "7999 loss on train 7.838775491109118e-05\n",
      "7999 loss on test 2.8371698135742918e-05\n",
      "8099 loss on train 6.34374882793054e-05\n",
      "8099 loss on test 2.2417360014514998e-05\n",
      "8199 loss on train 6.828835466876626e-05\n",
      "8199 loss on test 2.426748505968135e-05\n",
      "8299 loss on train 6.117604061728343e-05\n",
      "8299 loss on test 2.1509556972887367e-05\n",
      "8399 loss on train 6.077662328607403e-05\n",
      "8399 loss on test 2.1321540771168657e-05\n",
      "8499 loss on train 5.9043413784820586e-05\n",
      "8499 loss on test 2.066247907350771e-05\n",
      "8599 loss on train 5.8211080613546073e-05\n",
      "8599 loss on test 2.0314550056355074e-05\n",
      "8699 loss on train 5.7044198911171407e-05\n",
      "8699 loss on test 1.9880068066413514e-05\n",
      "8799 loss on train 5.78586186747998e-05\n",
      "8799 loss on test 2.0197154299239628e-05\n",
      "8899 loss on train 5.662452895194292e-05\n",
      "8899 loss on test 1.979574517463334e-05\n",
      "8999 loss on train 5.900572068640031e-05\n",
      "8999 loss on test 2.063068313873373e-05\n",
      "9099 loss on train 5.383654570323415e-05\n",
      "9099 loss on test 1.8618242393131368e-05\n",
      "9199 loss on train 5.297542520565912e-05\n",
      "9199 loss on test 1.8264550817548297e-05\n",
      "9299 loss on train 0.00015432552027050406\n",
      "9299 loss on test 5.8441444707568735e-05\n",
      "9399 loss on train 5.641720417770557e-05\n",
      "9399 loss on test 1.9646839064080268e-05\n",
      "9499 loss on train 4.967712447978556e-05\n",
      "9499 loss on test 1.6995965779642574e-05\n",
      "9599 loss on train 0.0004144228878431022\n",
      "9599 loss on test 0.0001590508036315441\n",
      "9699 loss on train 4.798591180588119e-05\n",
      "9699 loss on test 1.6332765881088562e-05\n",
      "9799 loss on train 4.719237767858431e-05\n",
      "9799 loss on test 1.6026013327063993e-05\n",
      "9899 loss on train 4.6831606596242636e-05\n",
      "9899 loss on test 1.5874615201028064e-05\n",
      "9999 loss on train 5.29571516381111e-05\n",
      "9999 loss on test 1.829669599828776e-05\n"
     ]
    }
   ],
   "source": [
    "# Train a net without angles\n",
    "t_plot1, lossplottrain1, lossplottest1,model1 = Net1(inp_np_train_U, inp_np_test_U, out_np_train, out_np_test,  Epochs, NmU, NmNut, angles_train_np, angles_test_np, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 loss on train 5.971549034118652\n",
      "99 loss on test 2.5760104656219482\n",
      "199 loss on train 0.16366219520568848\n",
      "199 loss on test 0.05496241897344589\n",
      "299 loss on train 0.06670722365379333\n",
      "299 loss on test 0.02291920594871044\n",
      "399 loss on train 0.04424139857292175\n",
      "399 loss on test 0.01668764464557171\n",
      "499 loss on train 0.034943588078022\n",
      "499 loss on test 0.013666072860360146\n",
      "599 loss on train 0.029168663546442986\n",
      "599 loss on test 0.011586438864469528\n",
      "699 loss on train 0.025034256279468536\n",
      "699 loss on test 0.010009683668613434\n",
      "799 loss on train 0.021206451579928398\n",
      "799 loss on test 0.008731338195502758\n",
      "899 loss on train 0.016993239521980286\n",
      "899 loss on test 0.0068952604196965694\n",
      "999 loss on train 0.015228915959596634\n",
      "999 loss on test 0.006147061008960009\n",
      "1099 loss on train 0.01392076350748539\n",
      "1099 loss on test 0.005573526956140995\n",
      "1199 loss on train 0.01213726494461298\n",
      "1199 loss on test 0.004849158227443695\n",
      "1299 loss on train 0.011355945840477943\n",
      "1299 loss on test 0.0045942761935293674\n",
      "1399 loss on train 0.011431596241891384\n",
      "1399 loss on test 0.004514459986239672\n",
      "1499 loss on train 0.011407840065658092\n",
      "1499 loss on test 0.004481815733015537\n",
      "1599 loss on train 0.0106409452855587\n",
      "1599 loss on test 0.004183221608400345\n",
      "1699 loss on train 0.008487673476338387\n",
      "1699 loss on test 0.003435376565903425\n",
      "1799 loss on train 0.009534541517496109\n",
      "1799 loss on test 0.0037308353930711746\n",
      "1899 loss on train 0.006563661620020866\n",
      "1899 loss on test 0.0026601168792694807\n",
      "1999 loss on train 0.006088059861212969\n",
      "1999 loss on test 0.002477506874129176\n",
      "2099 loss on train 0.01708715222775936\n",
      "2099 loss on test 0.006474438589066267\n",
      "2199 loss on train 0.007364416029304266\n",
      "2199 loss on test 0.002875689882785082\n",
      "2299 loss on train 0.004985997453331947\n",
      "2299 loss on test 0.00203875289298594\n",
      "2399 loss on train 0.010131915099918842\n",
      "2399 loss on test 0.004040584899485111\n",
      "2499 loss on train 0.004668545909225941\n",
      "2499 loss on test 0.0018804867286235094\n",
      "2599 loss on train 0.005170660093426704\n",
      "2599 loss on test 0.0020516144577413797\n",
      "2699 loss on train 0.005953045096248388\n",
      "2699 loss on test 0.0023359241895377636\n",
      "2799 loss on train 0.005832558497786522\n",
      "2799 loss on test 0.0022459044121205807\n",
      "2899 loss on train 0.0036100572906434536\n",
      "2899 loss on test 0.0014620329020544887\n",
      "2999 loss on train 0.0035289423540234566\n",
      "2999 loss on test 0.0014498578384518623\n",
      "3099 loss on train 0.003262761514633894\n",
      "3099 loss on test 0.0013381181051954627\n",
      "3199 loss on train 0.0031709102913737297\n",
      "3199 loss on test 0.0013089529238641262\n",
      "3299 loss on train 0.002926879795268178\n",
      "3299 loss on test 0.001190194976516068\n",
      "3399 loss on train 0.0030067258048802614\n",
      "3399 loss on test 0.0012408292386680841\n",
      "3499 loss on train 0.004707001149654388\n",
      "3499 loss on test 0.0019169666338711977\n",
      "3599 loss on train 0.0025204485282301903\n",
      "3599 loss on test 0.0010289091151207685\n",
      "3699 loss on train 0.0032518787775188684\n",
      "3699 loss on test 0.00128882413264364\n",
      "3799 loss on train 0.005812645424157381\n",
      "3799 loss on test 0.0023334489669650793\n",
      "3899 loss on train 0.0027642641216516495\n",
      "3899 loss on test 0.0011492528719827533\n",
      "3999 loss on train 0.006665140390396118\n",
      "3999 loss on test 0.0025412372779101133\n",
      "4099 loss on train 0.007203639019280672\n",
      "4099 loss on test 0.0027577183209359646\n",
      "4199 loss on train 0.003181158099323511\n",
      "4199 loss on test 0.0013168253935873508\n",
      "4299 loss on train 0.0028532901778817177\n",
      "4299 loss on test 0.0011932330671697855\n",
      "4399 loss on train 0.0017966240411624312\n",
      "4399 loss on test 0.0007538374629803002\n",
      "4499 loss on train 0.004527511075139046\n",
      "4499 loss on test 0.0017443787073716521\n",
      "4599 loss on train 0.0024616969749331474\n",
      "4599 loss on test 0.0009984741918742657\n",
      "4699 loss on train 0.001839467673562467\n",
      "4699 loss on test 0.0007663639262318611\n",
      "4799 loss on train 0.002934170188382268\n",
      "4799 loss on test 0.0012372429482638836\n",
      "4899 loss on train 0.0016458425670862198\n",
      "4899 loss on test 0.0006554299616254866\n",
      "4999 loss on train 0.005246161483228207\n",
      "4999 loss on test 0.0019234244246035814\n",
      "5099 loss on train 0.0050655547529459\n",
      "5099 loss on test 0.002046107081696391\n",
      "5199 loss on train 0.0018483659951016307\n",
      "5199 loss on test 0.0007090797298587859\n",
      "5299 loss on train 0.0032714740373194218\n",
      "5299 loss on test 0.0012199569027870893\n",
      "5399 loss on train 0.0020134523510932922\n",
      "5399 loss on test 0.0008377420017495751\n",
      "5499 loss on train 0.0017459978116676211\n",
      "5499 loss on test 0.0007208827300928533\n",
      "5599 loss on train 0.0016701251734048128\n",
      "5599 loss on test 0.000661119760479778\n",
      "5699 loss on train 0.010839652270078659\n",
      "5699 loss on test 0.004206206649541855\n",
      "5799 loss on train 0.006193105597048998\n",
      "5799 loss on test 0.0023358098696917295\n",
      "5899 loss on train 0.006364170461893082\n",
      "5899 loss on test 0.002482282230630517\n",
      "5999 loss on train 0.001702662673778832\n",
      "5999 loss on test 0.0006641963263973594\n",
      "6099 loss on train 0.0018524539191275835\n",
      "6099 loss on test 0.0007238042308017612\n",
      "6199 loss on train 0.009142209775745869\n",
      "6199 loss on test 0.0035163559950888157\n",
      "6299 loss on train 0.0037518832832574844\n",
      "6299 loss on test 0.0013782570604234934\n",
      "6399 loss on train 0.001987891737371683\n",
      "6399 loss on test 0.0008179086144082248\n",
      "6499 loss on train 0.05967883765697479\n",
      "6499 loss on test 0.021751832216978073\n",
      "6599 loss on train 0.008367444388568401\n",
      "6599 loss on test 0.0031763343140482903\n",
      "6699 loss on train 0.03456665575504303\n",
      "6699 loss on test 0.01268158107995987\n",
      "6799 loss on train 0.006149576045572758\n",
      "6799 loss on test 0.00219077430665493\n",
      "6899 loss on train 0.0025061913765966892\n",
      "6899 loss on test 0.0009555884171277285\n",
      "6999 loss on train 0.02075256034731865\n",
      "6999 loss on test 0.007534622680395842\n",
      "7099 loss on train 0.001071737613528967\n",
      "7099 loss on test 0.00039435073267668486\n",
      "7199 loss on train 0.00310793099924922\n",
      "7199 loss on test 0.0010949465213343501\n",
      "7299 loss on train 0.0029757260344922543\n",
      "7299 loss on test 0.0010997544741258025\n",
      "7399 loss on train 0.021738804876804352\n",
      "7399 loss on test 0.008114585652947426\n",
      "7499 loss on train 0.004711043555289507\n",
      "7499 loss on test 0.0016898376634344459\n",
      "7599 loss on train 0.015750205144286156\n",
      "7599 loss on test 0.005856781732290983\n",
      "7699 loss on train 0.0030773920007050037\n",
      "7699 loss on test 0.0010845905635505915\n",
      "7799 loss on train 0.008629916235804558\n",
      "7799 loss on test 0.003339359536767006\n",
      "7899 loss on train 0.0030191841069608927\n",
      "7899 loss on test 0.0010776284616440535\n",
      "7999 loss on train 0.006284948438405991\n",
      "7999 loss on test 0.002270099939778447\n",
      "8099 loss on train 0.002492843195796013\n",
      "8099 loss on test 0.0009803050197660923\n",
      "8199 loss on train 0.01653474010527134\n",
      "8199 loss on test 0.006073919124901295\n",
      "8299 loss on train 0.012496843002736568\n",
      "8299 loss on test 0.004646770656108856\n",
      "8399 loss on train 0.0032922346144914627\n",
      "8399 loss on test 0.0012228544801473618\n",
      "8499 loss on train 0.0044018980115652084\n",
      "8499 loss on test 0.00163074501324445\n",
      "8599 loss on train 0.002651139162480831\n",
      "8599 loss on test 0.0009692190797068179\n",
      "8699 loss on train 0.018060479313135147\n",
      "8699 loss on test 0.006667982321232557\n",
      "8799 loss on train 0.007760464213788509\n",
      "8799 loss on test 0.002851983532309532\n",
      "8899 loss on train 0.006264332681894302\n",
      "8899 loss on test 0.0023360340856015682\n",
      "8999 loss on train 0.004294695798307657\n",
      "8999 loss on test 0.0015687100822106004\n",
      "9099 loss on train 0.006449695210903883\n",
      "9099 loss on test 0.0023403880186378956\n",
      "9199 loss on train 0.006121461279690266\n",
      "9199 loss on test 0.0023140220437198877\n",
      "9299 loss on train 0.01192933227866888\n",
      "9299 loss on test 0.004471683409065008\n",
      "9399 loss on train 0.0055526020005345345\n",
      "9399 loss on test 0.002119378186762333\n",
      "9499 loss on train 0.000766402343288064\n",
      "9499 loss on test 0.00031076674349606037\n",
      "9599 loss on train 0.01560980360955\n",
      "9599 loss on test 0.005955011583864689\n",
      "9699 loss on train 0.002315916819497943\n",
      "9699 loss on test 0.0008652060059830546\n",
      "9799 loss on train 0.008056838996708393\n",
      "9799 loss on test 0.0029532143380492926\n",
      "9899 loss on train 0.0019382196478545666\n",
      "9899 loss on test 0.0007126926211640239\n",
      "9999 loss on train 0.029003294184803963\n",
      "9999 loss on test 0.010602032765746117\n"
     ]
    }
   ],
   "source": [
    "# Train a net with angles\n",
    "t_plot2, lossplottrain2, lossplottest2,model2 = Net1(inp_np_train_U, inp_np_test_U, out_np_train, out_np_test,  Epochs, NmU, NmNut, angles_train_np, angles_test_np, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_plot1, lossplottrain1, label=\"train1\")\n",
    "plt.plot(t_plot1, lossplottest1, label=\"test1\")\n",
    "plt.plot(t_plot2, lossplottrain2, label=\"train2\")\n",
    "plt.plot(t_plot2, lossplottest2, label=\"test2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_np_train_U[100,0:NmU].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model = model1(torch.from_numpy(inp_np_train_U[100,0:NmU]).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_torch = torch.from_numpy(out_np_train[100,0:NmNut]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -2.8978,   -3.6177,   -3.1317,  -53.0360,  -87.0661, -226.9347,\n",
       "           9.6484,   -4.6611,  -87.3613,  -50.2747], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_model - out_torch)/out_torch*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.2969e-03,  8.6416e-04,  2.7014e-03, -2.5519e-04,  7.9460e-06,\n",
       "        -1.8694e-05, -4.2051e-04,  3.2825e-04, -2.2680e-05,  1.2030e-04],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.4251e-03,  8.9659e-04,  2.7887e-03, -5.4338e-04,  6.1436e-05,\n",
       "         1.4727e-05, -3.8351e-04,  3.4430e-04, -1.7944e-04,  2.4193e-04])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
